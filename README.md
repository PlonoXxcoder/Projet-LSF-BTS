<table>
  <tr>
    <td><img src="components/assets/Logo.png" alt="Logo du projet" width="60"/></td>
    <td><h1>Traducteur LSF vers Texte et Audio</h1></td>
  </tr>
</table>

Ce dÃ©pÃ´t contient le projet de [BTS CIEL](https://www.onisep.fr/ressources/univers-formation/formations/post-bac/bts-cybersecurite-informatique-et-reseaux-electronique-option-a-informatique-et-reseaux) (CybersÃ©curitÃ©, Informatique et rÃ©seaux, Ã‰lectronique) visant Ã  dÃ©velopper un systÃ¨me de traduction de la [Langue des Signes FranÃ§aise (LSF)](https://fr.wikipedia.org/wiki/Langue_des_signes_fran%C3%A7aise) vers du texte et de l'audio. L'objectif principal est de faciliter la communication pour les personnes malentendantes et leur entourage. Le systÃ¨me comprend plusieurs modules: capture vidÃ©o, reconnaissance des signes, traduction, et synthÃ¨se vocale.

ğŸ‘‰ **TÃ©lÃ©chargez la derniÃ¨re version fonctionnelle dans la [section Releases](https://github.com/PlonoXxcoder/Projet-LSF-BTS/releases)**.

**ğŸ“ŒContexte du projet :**
*   **ğŸ“Formation :** BTS CIEL - [LycÃ©e Edouard Branly](https://branly.etab.ac-lyon.fr/spip/)
*   **ğŸ“šÃ‰preuve :** E6
*   **â³DurÃ©e :** 150 heures
*   **ğŸ‘©â€ğŸ’»Ã‰quipe :** 4 Ã©tudiants ([voir section RÃ´les](#Ã©quipe-et-rÃ©partition-des-rÃ´les))
*   **ğŸ‘¥Public Cible :** Personnes malentendantes et leur entourage

## ğŸ¯Objectifs GÃ©nÃ©raux du Projet 

1.  **ğŸ› ï¸DÃ©velopper un systÃ¨me de capture vidÃ©o** et de traitement d'image pour dÃ©tecter les mains et le visage.
2.  **ğŸ¤–ImplÃ©menter un module d'Intelligence Artificielle** pour la reconnaissance des signes LSF capturÃ©s.
3.  **ğŸ’¬CrÃ©er un module de traduction** pour convertir les signes reconnus en phrases franÃ§aises textuelles, en gÃ©rant la grammaire et la syntaxe.
4.  **ğŸ™ï¸IntÃ©grer un module de synthÃ¨se vocale** pour convertir le texte traduit en parole audible.
5.  **ğŸ–¥ï¸Concevoir une interface utilisateur graphique (GUI)** accessible et intuitive.
6.  **â±ï¸Assurer une traduction en temps rÃ©el ou quasi-rÃ©el** (latence < 2 secondes).
7.  *ğŸ¯*Atteindre une prÃ©cision de reconnaissance supÃ©rieure Ã  85%.**

## Installation
*Ã€ venir â€” en attendant, consultez la section [Release](https://github.com/PlonoXxcoder/Projet-LSF-BTS/releases) pour rÃ©cupÃ©rer une version fonctionnelle.*

## ğŸ› ï¸Technologies UtilisÃ©es

*   **ğŸPython 3.18.10**
*   **âš™ï¸TensorFlow / Keras:** Framework principal pour la construction et l'entraÃ®nement du modÃ¨le LSTM.
*   **ğŸ“¹MediaPipe:** (UtilisÃ© en amont) Pour l'extraction des points clÃ©s du corps et des mains Ã  partir des vidÃ©os. Ce script *ne fait pas* l'extraction, il utilise les rÃ©sultats.
*   **ğŸ–¼ï¸OpenCV (`opencv-python`):** UtilisÃ© potentiellement pour le traitement vidÃ©o en amont (non prÃ©sent dans ce script).
*   **ğŸ”¢NumPy:** Manipulation efficace des tableaux numÃ©riques (sÃ©quences de points clÃ©s).
*   **ğŸ“ŠScikit-learn:** Pour la division des donnÃ©es (`train_test_split`).
*   **âš–ï¸imbalanced-learn:** (Optionnel) Pour l'Ã©quilibrage des classes avec SMOTE.
*   **ğŸ›ï¸Keras Tuner:** Pour l'optimisation automatique des hyperparamÃ¨tres.
*   **ğŸ“ˆMatplotlib:** Pour la visualisation des graphiques d'entraÃ®nement.


## Workflow Global du SystÃ¨me

1.  **ğŸ“·Capture :** La camÃ©ra filme l'utilisateur signant (Module [@PlonoXxcoder](https://github.com/PlonoXxcoder)).
2.  **ğŸ”Extraction :** [MediaPipe](https://developers.google.com/mediapipe) dÃ©tecte et extrait les coordonnÃ©es (x, y, z) des points clÃ©s (Module [@PlonoXxcoder](https://github.com/PlonoXxcoder)).
3.  **âš™ï¸Normalisation/PrÃ©traitement :** Les sÃ©quences sont normalisÃ©es et formatÃ©es (Module [@PlonoXxcoder](https://github.com/PlonoXxcoder) ou [@aj69210](https://github.com/aj69210)).
4.  **ğŸ§ Reconnaissance :** Le modÃ¨le IA entraÃ®nÃ© prÃ©dit le signe LSF (Module [@PlonoXxcoder](https://github.com/PlonoXxcoder) et [@aj69210](https://github.com/aj69210)).
5.  **ğŸ“œTraduction :** Le(s) signe(s) est/sont traduit(s) en texte franÃ§ais.
6.  **ğŸ”ŠSynthÃ¨se Vocale :** Le texte est converti en audio (Module [@Rafael1101001](https://github.com/Rafael1101001)).
7.  **ğŸ–¥ï¸Affichage :** L'interface utilisateur montre vidÃ©o, texte, et/ou audio (Module [@Walid01100](https://github.com/Walid01100)).


## Ã‰quipe et RÃ©partition des RÃ´les

*   **ğŸ¥[@PlonoXxcoder](https://github.com/PlonoXxcoder) :** Responsable du module de capture vidÃ©o et traitement d'image.
*   **ğŸ¤–[@PlonoXxcoder](https://github.com/PlonoXxcoder) et [@aj69210](https://github.com/aj69210) :** Responsable du dÃ©veloppement IA et reconnaissance des signes.
*   **ğŸ¨[@Walid01100](https://github.com/Walid01100) :** Responsable de l'interface utilisateur et de l'expÃ©rience utilisateur.
*   **ğŸ”Š[@Rafael1101001](https://github.com/Rafael1101001) :** Responsable de la synthÃ¨se vocale et de l'intÃ©gration audio.

*Chaque personne est responsable de la documentation, des tests unitaires/d'intÃ©gration de sa partie et de la contribution Ã  la prÃ©sentation finale.*

## ğŸ¤Coordination de l'Ã‰quipe

*   ğŸ“…RÃ©unions hebdomadaires de suiviğŸ“‹.
*   ğŸ“Documentation collaborative sur WikiğŸ“š.



## ğŸ’¡Contribution

Nous sommes ouverts aux contributions pour amÃ©liorer ce projetğŸš€. Voici comment procÃ©der :

### Pour contribuer :
1. **ğŸ› ï¸Signaler un problÃ¨me ou suggÃ©rer une amÃ©lioration**  
   Ouvrez une [issue](https://github.com/PlonoXxcoder/Projet-LSF-BTS/issues) en dÃ©taillant :
   - Le contexteğŸ–¼ï¸
   - Les Ã©tapes pour reproduire le problÃ¨me (le cas Ã©chÃ©ant)â†©ï¸
   - Le comportement attenduğŸ¯

2. **Contribuer au code**  
   ```bash
   # 1. Forker le dÃ©pÃ´t
   # 2. Cloner votre fork
   git clone https://github.com/votre-utilisateur/Projet-LSF-BTS.git
   # 3. CrÃ©er une branche
   git checkout -b feature/ma-contribution
   # 4. AprÃ¨s modifications :
   git commit -m "Description claire des changements"
   git push origin feature/ma-contribution
   # 5. Ouvrir une Pull Request
   
**âš ï¸Bonnes pratiques :**
  - Respecter le style de code existantğŸ”§.
  - Documenter les nouvelles fonctionnalitÃ©sğŸ“˜.
  - Ajouter des tests unitaires le cas Ã©chÃ©antğŸ”.



## ğŸ“œLicence

Ce projet est open-source et distribuÃ© sous la [licence](https://github.com/PlonoXxcoder/Projet-LSF-BTS/blob/main/LICENSE)âœ… **MIT**.  

### âœ¨Vous Ãªtes autorisÃ© Ã  :
- ğŸš€Utiliser librement le logiciel
- ğŸ”§Modifier le code source
- ğŸ“¤Redistribuer des copies
- ğŸ’¼Utiliser Ã  des fins commerciales

### âš ï¸Conditions :
- ğŸ“„Inclure la notice de licence originale dans toutes les copies
- ğŸš«Ne pas tenir les auteurs responsables

_Pour plus de dÃ©tails, consultez le fichier [LICENSE](https://github.com/PlonoXxcoder/Projet-LSF-BTS/blob/main/LICENSE) complet._
## ğŸ™Remerciements

Nous tenons Ã  remercier :
- ğŸŒŸLes contributeurs du projet.
- ğŸ–¥ï¸Les crÃ©ateurs des bibliothÃ¨ques open-source utilisÃ©es dans ce projet.
- ğŸ“Nos professeurs et mentors pour leur soutien et leurs conseils.

## ğŸ†˜Support

Si vous rencontrez des problÃ¨mes ou avez des questions, n'hÃ©sitez pas Ã  ouvrir une issue sur GitHubğŸ’»ï¸ ou Ã  nous contacter directementâœ‰ï¸.


## FAQ

**Q : Comment puis-je exÃ©cuter le projet ?**
R : Suivez les instructions dans la section [Installation](#installation). Assurez-vous d'avoir Python 3.18.10 installÃ© et de crÃ©er un environnement virtuel pour gÃ©rer les dÃ©pendances.

**Q : Comment puis-je contribuer au projet ?**
R : Consultez la section [Contribution](#contribution) pour plus de dÃ©tails. Nous accueillons les contributions sous forme de nouvelles fonctionnalitÃ©s, corrections de bugs, ou amÃ©liorations de la documentation.

**Q : Quelles sont les technologies utilisÃ©es dans ce projet ?**
R : Le projet utilise principalement Python 3.18.10, TensorFlow/Keras pour l'IA, MediaPipe pour l'extraction des points clÃ©s, OpenCV pour le traitement vidÃ©o, NumPy pour la manipulation des tableaux, Scikit-learn pour la division des donnÃ©es, et Matplotlib pour la visualisation.

**Q : Comment puis-je tester le projet ?**
R : Chaque module dispose de tests unitaires et d'intÃ©gration. Vous pouvez exÃ©cuter ces tests en utilisant les commandes fournies dans la documentation de chaque module.

**Q : Le projet est-il compatible avec d'autres langues des signes ?**
R : Actuellement, le projet est conÃ§u pour la Langue des Signes FranÃ§aise (LSF). Cependant, l'architecture est suffisamment flexible pour Ãªtre adaptÃ©e Ã  d'autres langues des signes avec les donnÃ©es et modÃ¨les appropriÃ©s.

**Q : Comment puis-je signaler un bug ou suggÃ©rer une amÃ©lioration ?**
R : Vous pouvez ouvrir une issue sur GitHub en dÃ©crivant le problÃ¨me ou la suggestion de maniÃ¨re dÃ©taillÃ©e. Plus vous fournirez d'informations, plus il sera facile pour nous de traiter votre demande.

**Q : Le projet est-il open-source ?**
R : Oui, le projet est open-source et distribuÃ© sous la licence [MIT](LICENSE). Vous Ãªtes libre de l'utiliser, de le modifier et de le distribuer selon les termes de cette licence.

**Q : Comment puis-je configurer les variables d'environnement nÃ©cessaires ?**
R : CrÃ©ez un fichier `.env` Ã  la racine du projet et ajoutez-y les variables d'environnement nÃ©cessaires. Un exemple de fichier `.env` est fourni dans la documentation.

**Q : Le projet fonctionne-t-il en temps rÃ©el ?**
R : Oui, l'un des objectifs du projet est de fournir une traduction en temps rÃ©el ou quasi-rÃ©el avec une latence infÃ©rieure Ã  2 secondes.

**Q : Quelle est la prÃ©cision de reconnaissance du modÃ¨le ?**
R : Nous visons une prÃ©cision de reconnaissance supÃ©rieure Ã  85%. Les performances actuelles peuvent varier en fonction des donnÃ©es d'entraÃ®nement et des conditions d'utilisation.

**Q : Comment puis-je obtenir de l'aide si je rencontre des problÃ¨mes ?**
R : Vous pouvez ouvrir une issue sur GitHub ou nous contacter directement. Nous ferons de notre mieux pour vous aider Ã  rÃ©soudre vos problÃ¨mes.

